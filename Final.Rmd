---
title: "STAT 380 Final Project"
author: "Brian Ellis and James Tondt"
date: "2023-04-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

## Front Matter
```{r}
rm(list = ls())
#Add libraries as needed
library(tidyverse)
library(randomForest)
library(lubridate)
library(glmnet)
library(FNN)
library(e1071)
```

## Call of Duty

![CODGames](Call-of-Duty.jpg-d196774.png)

```{r}
# Load datasets
P1 <- read_csv(file = "CODGames_p1_380.csv", show_col_types = FALSE)
head(P1)

P2 <- read_csv("CODGames_p2_380.csv", show_col_types = FALSE)
head(P2)

Maps <- read_csv("CODMaps.csv")
head(Maps)

Modes <- read_csv("CODGameModes.csv")
Modes
```

```{r}
# Appending Player 2 df to Player 1 df
Players <- P1 %>% 
  rbind(P2)
head(Players)
```

## Task 1 (Data Cleaning and Visualization)

Research Question: Which maps are the most likely to win the map vote? 

Since this question is centered around the probability of a map winning the map vote, we will not consider those maps which have null values in either the Map1 or Map2 columns, this includes the games which the player entered partway through. We want to judge only the rows where a vote took place, therefore we will omit any NA values in the Map1 or Map2 columns at the start of our analysis as well as any NA values in the Choice column. We will then get to know our data by listing out each unique map name in Choice, Map1, and Map2 and comparing them to the actual names found in our Maps data frame. If we detect any discrepancies we will have to account for this in our data cleaning step. Once each name is cleaned, we can perform the calculations across the variables. We will count the number of times a map name appears in both Map1 and Map2, the amount of times they were chosen in both Map1 and Map2, the number of Draws of Map Vote and which columns this took place in. It is incredibly important throughout this process that order be maintained throughout the rows so that when we eventually merge the Draw vector it will be placed in the proper row. From this information we will be able to subtract 'wins' from those maps in the Map1 row where a draw occurred and discount losses for the maps in Map2 column where a draw occurred. Finally, we'll be able to calculate the proportion of times a map won the map vote by getting more votes than the other and conclude from that information which maps are most likely to win the map vote.

```{r}
# Compare map names of Choice to actual Map names
unique(Players$Choice)
unique(Maps$Name)
```

We can see that there are more unique map names in our Players data frame as compared with the list of all the map names from our Maps data frame. This shows us there are misspellings in the map names in the Players data frame that we must correct before calculating summary statistics.

```{r}
Maps %>% 
  full_join(Players, by = c("Name" = "Choice")) %>% 
  group_by(Name) %>% 
  summarize(Count = n())
```


Using this join query, we can see there are multiple misspellings, added spaces, and a couple NA values. We shall correct for all this before running summaries.

```{r}
# Choice Data Cleaning
# remove NAs from Choice, Map1, and Map2
Players <- Players %>% 
  drop_na(Choice) %>%  
  drop_na(Map1) %>% 
  drop_na(Map2)
# Remove trailing or leading spaces
Players <- Players %>% 
  mutate(Choice = trimws(Choice),
         Map1 = trimws(Map1),
         Map2 = trimws(Map2))

# Define a list of misspellings and their correct spellings
misspellings <- list("APocalypse" = "Apocalypse", "Apocolypse" = "Apocalypse", "\\bCollaterel Strike\\b" = "Collateral", "\\bCollateral Strike\\b" = "Collateral", "Deisel" = "Diesel", "Drive-in" = "Drive-In", "\\bMiami Strike\\b" = "MiamiStrike", "\\bNuketown\\b.*" = "Nuketown", "Riad" = "Raid")

# Loop through the misspellings list and apply regex to correct the misspellings
for (misspelling in names(misspellings)) {
  correction <- misspellings[[misspelling]]
  Players$Choice <- gsub(misspelling, correction, Players$Choice, ignore.case = TRUE)
}
# Run new query to see if misspellings were corrected
Players %>% 
  group_by(Choice) %>% 
  summarise(Count = n())
```

We ended up changing some map names, e.g., "Collateral Strike" is now just "Collateral" and "Miami Strike" is "MiamiStrike". We needed to make these map names single words because otherwise the loop would not process or recognize the names correctly once they were altered.

Our misspellings are corrected and we can perform this data cleaning process on Map1 and Map2 variables now

```{r}
sum(is.na(Players$Map1))
sum(is.na(Players$Map2))
```

Now we will perform the same data cleaning steps with Map1 and Map2.

```{r}
# Compare names of Map1 and correct map names
Maps %>% 
  full_join(Players, by = c("Name" = "Map1")) %>% 
  group_by(Name) %>% 
  summarize(Count = n())
```

Since there are new typos, we just need to adjust our misspellings list to account for these. Note, we use the '\\\b' encasing to denote we want to only replace the value when it matches exactly that spelling.

```{r}
# Map1 Data Cleaning
misspellings <- list("\\bCollateral Striek\\b" = "Collateral", "\\bCollateral Strike\\b" = "Collateral", "\\bCollaterol Strike\\b" = "Collateral", "Deprogam" = "Deprogram", "Drive-in" = "Drive-In", "\\bMiami Stirke\\b" = "MiamiStrike", "\\bMiami Strike\\b" = "MiamiStrike", "\\bNuketown '84\\b" = "Nuketown", "Riad" = "Raid", "Ruah" = "Rush")

# Correct misspellings
for (misspelling in names(misspellings)) {
  correction <- misspellings[[misspelling]]
  Players$Map1 <- gsub(misspelling, correction, Players$Map1, ignore.case = TRUE)
}
# Run new query to see if misspellings were corrected
Players %>% 
  group_by(Map1) %>% 
  summarise(Count = n())
```

Great, now we just have to clean the names of Map2 and we can begin performing visualizations and running summary statistics.

```{r}
# Compare Map2 names with actual names
Maps %>% 
  full_join(Players, by = c("Name" = "Map2")) %>% 
  group_by(Name) %>% 
  summarize(Count = n())
```

```{r}
# Map2 Data Cleaning
misspellings <- list("\\bAmrada Strike\\b" = "Armada Strike", "\\bCollateral Strike\\b" = "Collateral", "Drive-in" = "Drive-In", "\\bMiami Sstrike\\b" = "MiamiStrike", "\\bMiami Stirke\\b" = "MiamiStrike", "\\bMiami Strike\\b" = "MiamiStrike", "\\bNuketown\\b.*" = "Nuketown", "\\byamantau\\b" = "Yamantau")

# Loop
for (misspelling in names(misspellings)) {
  correction <- misspellings[[misspelling]]
  Players$Map2 <- gsub(misspelling, correction, Players$Map2, ignore.case = TRUE)
}
# Run new query to see if misspellings were corrected
Players %>% 
  group_by(Map2) %>% 
  summarise(Count = n())
```

Now that our data is cleaned and variables match each other, we can begin visualizing the data.

```{r}
# Visualization of Choice
Players %>% 
  ggplot(aes(x = Choice)) +
  geom_histogram(stat = "count") +
  labs(title = "Distribution of Choice",
       x = "Choice",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


```{r}
# Visualization of Map 1
Players %>% 
  ggplot(aes(x = Map1)) +
  geom_histogram(stat = "count") +
  labs(title = "Distribution of Map 1",
       x = "Map",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# Visualization of Map 2
Players %>% 
  ggplot(aes(x = Map2)) +
  geom_histogram(stat = "count", na.rm = TRUE) +
  labs(title = "Distribution of Map 2",
       x = "Map",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

From these visualizations, we can verify that the map names do not change but the distributions do. Our data cleaning is complete and we can now move onto calculating proportions.

Now we need to work with the 'MapVote' variable and identify when a tie takes place. This is important, because whenever there is a tie, 'Map1' gets chosen by default. We will first alter the 'MapVote' variable into a numerical form where we can compare the two numbers in the column between each other and store each time the two numbers are equivalent.

```{r}
# Remove NAs from Mapvote
Map_vote <- Players$MapVote
# Split Mapvote
Vote_split <- strsplit(Map_vote, "to")

# Initialize storage
First_vote <- rep(NA, length(Vote_split))
Second_vote <- rep(NA, length(Vote_split))
Draw <- rep(NA, length(Vote_split))

# Loop through Vote_split and store first and second vote numbers
for (i in 1:length(Vote_split)) {
  First_vote[i] <- (Vote_split[[i]][1])
  Second_vote[i] <- (Vote_split[[i]][2])
}

print(First_vote[500:600])
print(length(First_vote))
print(length(Second_vote))
```

We see that there are some entries that do not represent numbers and will fail to convert to numeric values if we were to try now. We must impute these values and figure out there true meaning. The two entries in question from our vector First_vote are "4 o 0" and "2 o 0". This relates to a fundamental problem in our strsplit. The word "to" is misspelled as "o", meaning that these numbers will not be recognized by our split.

Now we know we must fix our strsplit function to cover these two cases.

```{r}
# Split Mapvote by "to" or "o"
Vote_split <- strsplit(Map_vote, "to|o")

# Remove NA values
# Vote_split <- na.omit(Vote_split) <- Don't know if this is necessary

# Initialize storage
First_vote <- rep(NA, length(Vote_split))
Second_vote <- rep(NA, length(Vote_split))
Draw <- rep(NA, length(Vote_split))

# Loop through Vote_split and store first and second vote numbers
for (i in 1:length(Vote_split)) {
  First_vote[i] <- (Vote_split[[i]][1])
  Second_vote[i] <- (Vote_split[[i]][2])
}

print(First_vote[500:600])
print(length(First_vote))
print(length(Second_vote))
```

Now we can't find the previous misspelled values and our number of rows are still same for our vectors.

We can compare the two vectors and count the number of times they match which would be equivalent to a draw. It is important that we keep the original order of our data frame so that we can know which rows ended in draws. We can then append the draw vector to our original data frame.

We now arrive at the number of times there was a draw in map votes between two maps. We know that whenever this occurs, 'Map1' is chosen by default. We can take this into consideration during our calculations.

Whenever the map vote results in a tie, we should discard the result of 'Choice' and not consider it as a win. We need to loop through all rows in the data frame Players and compare the map vote totals. If there is a tie, we must note this and discard 'Choice', otherwise, 'Choice' correctly depicts the winner. We must make proportions by summing up the total number of times a map was present in either 'Map1' or 'Map2' and its winning percentage. We may or may not discard the NA values of 'Map1' and 'Map2' because we don't know which maps the winner was competing against, but let's see.

```{r}
# Convert to numeric for comparison sake
First_vote <- as.numeric(First_vote)
Second_vote <- as.numeric(Second_vote)
# Loop through Vote split and store 1 to Draw for every equal vote total, 0 otherwise
for (i in 1:length(Vote_split)) {
  # If the first vote has NA values, the second vote vector will as well
  if (is.na(First_vote[i])) {
    Draw[i] <- 0
  }
  # If the numbers in first vote and second vote are equivalent, then store a 1 in Draw vector
  else if (First_vote[i] == Second_vote[i]) {
    Draw[i] <- 1
  }
  else {
    Draw[i] <- 0
  }
}
```

```{r}
Players <-
  Players %>% 
  mutate(Draw = Draw)

Players %>% 
  select(MapVote, Draw) %>% 
  head()
```

After adding the Draw vector, we see it correctly indicates a 1 in the 4th row where there is a draw in the MapVote column. Now that we have all our variables and data clean, we can perform the calculations:

```{r}
# First we summarize the totals from Map1 and Map2 and join them together
MapVoteWin <- Players %>%
  group_by(Map1) %>%
  summarize(Map1_Count = n(), Choice_M1 = sum(Choice == Map1), Draw_M1 = sum(Draw), Real_Win = Choice_M1 - Draw_M1) %>%
  left_join(
    Players %>%
      group_by(Map2) %>%
      summarize(Map2_Count = n(), Choice_M2 = sum(Choice == Map2), Draw_M2 = sum(Draw), Real_Loss = Map2_Count - Draw_M2),
    by = c("Map1" = "Map2")
  ) %>%
  # Next we create summary variables  
  mutate(
    Total_Count = Map1_Count + Map2_Count,        # Total amount of times Map appeared as potential Choice
    Total_Choice = Choice_M1 + Choice_M2,         # Total amount of times Map was Chosen
    Total_Draw = Draw_M1 + Draw_M2,               # Total Draws of Map
    Prop_Choice = Total_Choice / Total_Count,     # Selection proportion
    Adjusted_Choice_Count = Real_Win + Choice_M2, # Total Count with default wins by Draw removed
    Adjusted_Prop_Choice = Adjusted_Choice_Count / (Map1_Count + Real_Loss)   # Proportion of the times Map gained more votes than its competitor
  ) %>% 
  rename(Map = Map1) %>% 
  na.omit()
MapVoteWin
```

```{r}
MapVoteWin %>% 
  select(Map, Total_Count, Total_Choice, Adjusted_Choice_Count, Prop_Choice, Adjusted_Prop_Choice) %>% 
  arrange(desc(Adjusted_Prop_Choice))
```

From this analysis, we can see that the 3 maps most likely to win the map vote are Raid, Nuketown, and Crossroads Strike with true winning percentages of 75.6%, 73.7%, and 73.2% respectively. Nuketown actually had the highest proportion of Choice selection, with 82.5%, however it drew in map voting 5 times when it was 'Map1' and thus was selected 5 times by default when there was a tie in voting, greatly boosting its 'Choice' proportion. This is why we created the variables of 'Real_Win' and 'Real_Loss' to account for the number of times a map in 'Map1' column truly won the map vote and the number of times a map in 'Map2' actually lost the map vote and not lost by default because of a tie.

We can visualze this data:

```{r}
MapVoteWin %>% 
  ggplot(aes(x = reorder(Map, -Adjusted_Prop_Choice), fill = "dodgerblue")) +
  geom_bar(aes(weight = Adjusted_Prop_Choice), show.legend = F) +
  scale_fill_manual(values = c("dodgerblue")) +
  labs(title = "Probability to Win Map Vote",
       x = "Map",
       y = "Proportion of Map Vote Win") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

We visualize the adjusted proportion of choice by map and can see that Raid, Nuketown, and Crossroads Strike are the maps with the highest probability of winning the map vote.

# Task 2

```{r}

# Clean the data to clear to distinction between HC-TDM/Hardpoint and TDM/Hardpoint

Players <- 
  Players %>%
  mutate(GameType = str_replace(GameType, "HC - ", "")) 

# Check to see if data was properly cleaned and if there are any NAs
Players %>% 
  group_by(GameType) %>% 
  summarise(Count = n())

```


The GameType column has been cleaned so we can now start an exploratory data analysis. 


```{r}
ggplot(Players,
       aes(x = Score, y = TotalXP, color = GameType)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = "lm", se = F) +
  labs(title = "Total experience points vs Score by Game Type",
       y = "Total experience points")

```



```{r}
ggplot(Players, aes(x = GameType, y = TotalXP)) +
  geom_boxplot(fill = "blue", color = "black") +
  labs(title = "Distribution of Total experience points by Game Type",
       x = "Game Type")

ggplot(Players, aes(x = GameType, y = Score)) +
  geom_boxplot(fill = "blue", color = "black") +
  labs(title = "Distribution of Score by Game Type",
       x = "Game Type")

```

```{r}

```



```{r}
model1 <- lm(TotalXP ~ Score + GameType, data = Players)
summary(model1)

```


## Task 3

Our Research Question: Can we accurately predict weapon type based on several factors?

We will have to perform some data cleaning initially to make sure each weapon is accounted for and does not have typos. We then will decide on a threshold of the number of games a weapon must have been used in to be considered, too few games will not be convincing enough. We will be removing partial matches from consideration to keep each observation independent and identically distributed. We will also create a new date variable, most likely around months, which will give us information as to whether the weapons are more prevelant during certain time periods. After this data cleaning process, we will begin feature selection for our models. We will probably build a LASSO feature selection model which can help us determine which features are most important for predicting 'PrimaryWeapon'. When comparing the different models, we must compare them on the same metric which we have determined will be accuracy. We will be using the same random seed of 42 throughout this model process to keep our results replicable and consistent throughout the models. The three models we will be implementing are random forest, kNN classification, and Naive Bayes.

We'll begin by cleaning our data, doing some initial visualizations to explore the relationship between possible features and our target 'PrimaryWeapon', and creating a new date variable.

```{r}
# Remove partial matches
Players <-
  Players %>% 
  filter(FullPartial == "Full")
```


```{r}
# Summarize count of games each weapon appeared in
Players %>% 
  group_by(PrimaryWeapon) %>% 
  summarize(N = n())
```

We notice that there are some typos in a couple guns and multiple NA values which we must clean up before moving forward.

```{r}
# Drop NA values
Players <- Players %>% 
  drop_na(PrimaryWeapon)

# Correct typos
misspellings <- list("\\bMilano 821\\b" = "Milano", "\\bPellington\\b" = "Pelington 703")

# Loop through the misspellings list and apply regex to correct the misspellings
for (misspelling in names(misspellings)) {
  correction <- misspellings[[misspelling]]
  Players$PrimaryWeapon <- gsub(misspelling, correction, Players$PrimaryWeapon, ignore.case = TRUE)
}

# View corrections
Players %>% 
  group_by(PrimaryWeapon) %>% 
  summarize(N = n())
```

Now we will select only the weapons which appear in more than 50 games to reduce the number of classes for our classifcation models.

```{r}
# Remove weapons with few game counts
Weapons <- Players %>% 
  group_by(PrimaryWeapon) %>% 
  filter(n() >= 40)

Weapons %>% 
  group_by(PrimaryWeapon) %>% 
  summarise(N = n())
```

We see that there are 5 weapons which were used in 50 games or more, these will be the main classes of weapons which we will try and predict with our models. Our main intuition about these weapons is that they should differ in terms of 'Eliminations' and 'Damage'. Let's see if our intuition is correct with some visualzations:

```{r}
# Visualzing Eliminations for each Weapon
Weapons %>% 
  ggplot(aes(x = PrimaryWeapon, y = Eliminations)) +
  geom_boxplot() +
  labs(title = "Eliminations by Primary Weapon",
       x = "Primary Weapon",
       y = "Eliminations") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

```{r}
# Visualzing Damage for each Weapon
Weapons %>% 
  ggplot(aes(x = PrimaryWeapon, y = Damage)) +
  geom_boxplot() +
  labs(title = "Damage by Primary Weapon",
       x = "Primary Weapon",
       y = "Damage")
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

While 'Eliminations' doesn't give us a lot of information, 'Damage' seems like the variable which has a higher variance amongst the different weapons.

```{r}
Weapons %>% 
  ggplot(aes(x = Damage, y = Eliminations, color = PrimaryWeapon)) +
  geom_point() +
  labs(title = "Eliminations and Damage by Game Type") +
  facet_wrap( ~ GameType)
```

Let's make use of the date variable. We'll put it into a format that's useful, splitting it up by the month.

```{r}
# Create Month variable
Weapons <-
  Weapons %>% 
    mutate(Month = month(as.POSIXlt(Date, format = "%m/%d/%Y"))) %>% 
  mutate(Month = month.name[Month])

# Convert to continuous variable
Weapons$Month <- as.integer(factor(Weapons$Month, levels = month.name))
```

Now we can plot the changes over time for our weapons.

```{r}
Weapons %>%
  group_by(Month, PrimaryWeapon) %>%
  summarise(mean_eliminations = mean(Eliminations)) %>%
  ggplot(aes(x = Month, y = mean_eliminations, color = PrimaryWeapon)) +
  geom_line(aes(group = PrimaryWeapon)) +
  geom_point() +
  labs(x = "Month", y = "Eliminations", color = "Primary Weapon") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 8)) +
  scale_x_discrete(limits = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"))
```

We see that each weapon isn't present constantly over the months, showing that the players switch weapons constantly over time. This should make our new variable 'Month' important towards predicting weapon type.

### Feature Selection

Before we begin building our models, it would be beneficial to employ a feature selection model to help us determine which variables are most important in classifying our 'PrimaryWeapons'. Let's build a LASSO model to help us identify those variables most important to predicting 'PrimaryWeapon'. 

```{r}
# Create factor for PrimaryWeapon
Weapons$PrimaryWeapon <- factor(Weapons$PrimaryWeapon)

# Build LASSO model with most variables
formula <- as.formula("PrimaryWeapon ~ Eliminations + Deaths + TotalXP + Score + Damage + Month + GameType + Map1 + Map2 + Choice + XPType + GameType")

# Put data frame in form needed for glmnet
Xmat <- model.matrix(formula, data = Weapons)
y <- as.numeric(Weapons$PrimaryWeapon)

set.seed(42)
cv.out <- cv.glmnet(x = Xmat, y = y,
                    alpha = 1, standardize = TRUE,
                    nfolds = 10)
set.seed(NULL)
plot(cv.out)
```

Let's select the largest lambda value within 1 standard deviation of the minimum. This will provide us with a lambda value that will apply more regularization and thus narrow down our features to only the most important.

```{r}
bestlam1 <- cv.out$lambda.1se
#Predict the responses for the test set (use for MSE/RMSE calc)
lasso.pred1 <- predict(cv.out , s = bestlam1,
                      newx = Xmat)
#Find the coefficients
lasso.coef1 <- predict(cv.out , s = bestlam1,
                      type = "coefficients")
bestlam1
lasso.coef1
```

We see that the 'Month' variable dominates the model, while interestingly enough 'Eliminations' and 'Deaths' are discarded by this LASSO model telling us that they may not be that important. Let's try the model again but without the 'Month' or Map variables.

```{r}
# Build LASSO model
formula <- as.formula("PrimaryWeapon ~ Eliminations + Deaths + TotalXP + Score + Damage + GameType")

# Put data frame in form needed for glmnet
Xmat <- model.matrix(formula, data = Weapons)
y <- as.numeric(Weapons$PrimaryWeapon)

set.seed(42)
cv.out <- cv.glmnet(x = Xmat, y = y,
                    alpha = 1, standardize = TRUE,
                    nfolds = 10)
set.seed(NULL)

bestlam1 <- cv.out$lambda.1se
#Predict the responses for the test set (use for MSE/RMSE calc)
lasso.pred1 <- predict(cv.out , s = bestlam1,
                      newx = Xmat)
#Find the coefficients
lasso.coef1 <- predict(cv.out , s = bestlam1,
                      type = "coefficients")
bestlam1
lasso.coef1
```

We can see that the variables most important for predicting 'PrimaryWeapon' are 'Month', 'Eliminations', 'Score', 'Damage', and 'GameType' based on this LASSO model.

Now that we have determined the most important features, we will begin building our models on these variables.

### Random Forest Model

The first model we will implement will be a random forest model which is one of the techniques we learned in class. The approach of a random forest model relies on randomness of multiple decision trees and combines the predictions of all the trees to arrive at the most accurate prediction. The inclusion of this randomness in each tree helps to reduce overfitting and ensure each tree is different.

```{r}
# Create indicator variables for GameType and Month
Weapons <- Weapons %>%
  mutate(Month = factor(Month, levels = unique(Month)),
         GameType = factor(GameType, levels = unique(GameType))) %>% 
  mutate(Month_dummy = as.numeric(Month),
         Type_dummy = as.numeric(GameType)) # convert Month to numeric values

# Scale x variables
xvars <- c("Month_dummy", "Type_dummy", "Eliminations", "Score", "Damage")
Weapons[ , xvars] <- scale(Weapons[ , xvars], center = TRUE, scale = TRUE)
```


```{r}
# Generate Train and Test sets
set.seed(42)
train_ind <- sample(1:nrow(Weapons), floor(0.8 * nrow(Weapons)))
set.seed(NULL)

Train <- Weapons[train_ind, ]
Test <- Weapons[-train_ind, ]
```


```{r}
#Build model using a seed
set.seed(42)
rf <- randomForest(as.factor(PrimaryWeapon) ~ Eliminations + Score + Damage + GameType + Month, 
                   data = Train, 
                   ntree = 500, 
                   mtry = 5,
                   importance = TRUE)
set.seed(NULL)

#Obtain predicted probabilities - not need for confusion matrix but included to show we all get same probabilities with same seed
pred_prob <- predict(rf, newdata = Test, type = "prob")

# Get predictions
pred_weapon <- predict(rf, newdata = Test, type = "response")

# Calculate accuracy
rf_acc <- mean(pred_weapon == Test$PrimaryWeapon)
print(rf_acc)

# Parameters and Confusion matrix
print(rf)

rf$importance

plot(rf)

varImpPlot(rf, n.var = 5)
```

We achieve 86.2% accuracy based from our random forest model and see that 'Month' and 'Damage' are the most important variables for predicting 'PrimaryWeapon' based on the mean decrease in accuracy and Gini. This makes intuitive sense because each gun can have different damage settings and the player can have phases in their experience where they use one gun for a certain period of time before moving on. We also see that the error for our random forest model converges before 100 trees, showing that 500 trees aren't necessary. We can try and adjust the parameters of our random forest model to see if we can achieve a higher accuracy, but let's move onto another classification method first.

### KNN Classification

Our next classification method that we will implement is the k-Nearest Neighbors algorithm, which we covered in class. The basis behind the k-Nearest Neighbors algorithm is using Euclidean distance, or any other distance calculation, to classify new data to the class that is most common among the k nearest neighbors. Since our data is already scaled and split into training and testing sets, we can jump right into the algorithm. We will also make sure to test the effectiveness of this model on the same statistic as our random forest model which was accuracy. However, we don't know which value of k gives us the highest accuracy. This is where we will implement a loop to find the optimal k value.

```{r}
# Initialize storage vars
maxK <- 75
acc_vec <- rep(NA, maxK)
actual <- Test$PrimaryWeapon  # actual values
# Loop
for (k in 1:maxK) {
  
#Build kNN classification model
knn_res <- knn(train = Train[ , xvars, drop = FALSE],
               test = Test[ , xvars, drop = FALSE],
               cl = Train$PrimaryWeapon,
               k = k)

# calculate accuracy of kNN model
predicted <- knn_res  # predicted values
# Convert factor levels to match those of Test$PrimaryWeapon
predicted <- factor(predicted, levels = levels(Test$PrimaryWeapon))

# Calculate and store accuracy
acc_vec[k] <- sum(predicted == Test$PrimaryWeapon) / length(Test$PrimaryWeapon)  # calculate accuracy
}
```

With our accuracy stored in acc_vec, we can plot the accuracy and find the optimal k value.

```{r}
# To use ggplot, create a data frame
temp_df <- data.frame(k = 1:maxK, accuracy = acc_vec)

# Create plot
ggplot(temp_df, aes(x = k, y = acc_vec)) +
  geom_line() +
  labs(x = "Number of Nearest Neighbors (k)",
       y = "Accuracy")

# Best K value
which.max(acc_vec)
# Best accuracy
acc_vec[which.max(acc_vec)]
```

The best accuracy achieved was 81.5% with a K value of 3. Our random forest model outperforms this kNN model by about 5% in terms of accuracy.

### Naive Bayes Classifier

We will implement a naive bayes classifier for our final model. A Naive Bayes works by calculating the probability of a new data point belonging to each class based on the probability of its features given each class. Just like we did with our kNN classifier, we will loop through an important hyperparameter looking to optimize the accuracy value of the model. For our naive bayes classifier, this hyperparameter is the laplace value which adjusts the amount of smoothing applied to the model - a higher value results in more smoothing.

```{r}
# fit a naive bayes classifier to the training data
nb_model <- naiveBayes(as.factor(PrimaryWeapon) ~ Eliminations + Score + Damage + GameType + Month, data = Train)

# make predictions on the test data using the fitted model
nb_pred <- predict(nb_model, newdata = Test)

# calculate the accuracy of the model
nb_accuracy <- sum(nb_pred == Test$PrimaryWeapon) / nrow(Test)
```

```{r}
# Set up vector of laplace values to test
laplace_values <- seq(-2, 1, by = 0.1)

# Create an empty vector to store accuracy values
accuracy_values <- numeric(length = length(laplace_values))

# Loop through laplace values and fit naive bayes models
for (i in seq_along(laplace_values)) {
  nb_model <- naiveBayes(PrimaryWeapon ~ Eliminations + Score + Damage + GameType + Month, 
                         data = Train, 
                         laplace = laplace_values[i])
  
  # Make predictions on test set
  predicted <- predict(nb_model, newdata = Test)
  
  # Calculate accuracy
  accuracy_values[i] <- sum(predicted == Test$PrimaryWeapon) / length(predicted)
}

# Find the optimal laplace value based on highest accuracy
optimal_laplace <- laplace_values[which.max(accuracy_values)]

# Fit final model with optimal laplace value
nb_model <- naiveBayes(as.factor(PrimaryWeapon) ~ Eliminations + Score + Damage + GameType + Month, 
                       data = Train, 
                       laplace = optimal_laplace)
```

```{r}
# create a data frame with laplace_values and corresponding accuracies
acc_df <- data.frame(laplace_values, accuracy_values)

# plot accuracy vs. laplace_values
ggplot(acc_df, aes(x = laplace_values, y = accuracy_values)) + 
  geom_line() + 
  geom_point() + 
  labs(title = "Accuracy vs. Laplace Values", 
       x = "Laplace Values", 
       y = "Accuracy")

# Best accuracy
accuracy_values[which.max(accuracy_values)]
```

We find that a Laplace value of -1 gives us the highest accuracy of 81.5% for our naive bayes model. There are multiple laplace values below -1 which give us this accuracy but we chose -1 to prevent possible overfitting if we were to implement this model on new data.

After looking at 3 different classification methods, our random forest model produced the highest accuracy with 86.2%. Now that we have a general idea of which model performs better, we can optimize our chosen model which in our case is the random forest. We can loop through the hyperparameters of the model and choose the values which give us the best accuracy on our model. The hyperparameters that we will be looking to optimize in our random forest model are the number of trees (ntree) and the number of variables random sampled as candidates in each split (mtry).

```{r}
set.seed(42)

# Define ranges of parameter values
ntrees_range <- seq(50, 200, 50)
mtry_range <- seq(1, 5, 1)

# Initialize variables to store best parameter values and accuracy
best_ntrees <- 0
best_mtry <- 0
best_accuracy <- 0

# Nested for loop to iterate over parameter values
for (ntrees in ntrees_range) {
  for (mtry in mtry_range) {
    
    # Fit model using current parameter values
    looped_rf <- randomForest(PrimaryWeapon ~ Eliminations + Score + Damage + GameType + Month, 
                             data = Train, 
                             ntree = ntrees, 
                             mtry = mtry, 
                             importance = TRUE)
    
    # Calculate accuracy of model
    accuracy <- sum(predict(looped_rf, Test) == Test$PrimaryWeapon) / nrow(Test)
    
    # Check if current accuracy is better than previous best accuracy
    if (accuracy > best_accuracy) {
      best_accuracy <- accuracy
      best_ntrees <- ntrees
      best_mtry <- mtry
    }
  }
}

# Fit model with best parameter values and print results
Final_rf_model <- randomForest(as.factor(PrimaryWeapon) ~ Eliminations + Score + Damage + GameType + Month, 
                               data = Train, 
                               ntree = best_ntrees, 
                               mtry = best_mtry, 
                               importance = TRUE)


# Get predictions
pred_weapon_final <- predict(Final_rf_model, newdata = Test, type = "response")

# Calculate accuracy
final_accuracy <- mean(pred_weapon_final == Test$PrimaryWeapon)
print(final_accuracy)

set.seed(NULL)

# Best parameters and confusion matrix
print(Final_rf_model)
```

We see that the best accuracy we can achieve by maximizing the hyperparameters in our random forest model is 87.7%, showing that with the optimal parameters we increased our model accuracy by 1.5%. The optimal parameters for our final random forest model are 2 variables at each split and 50 trees.

```{r}
Final_rf_model$importance

plot(Final_rf_model)

varImpPlot(Final_rf_model, n.var = 5)
```

After implementing 3 different classification models -- random forest, kNN, Naive Bayes -- we conclude that our random forest model is the best for predicting 'PrimaryWeapon' based on the factors of 'Damage', 'Eliminations', 'Score', 'GameType', and 'Month' with an accuracy of 87.7%.

