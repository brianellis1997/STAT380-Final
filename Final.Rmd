---
title: "STAT 380 Final Project"
author: "Brian Ellis and James Tondt"
date: "2023-04-22"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter
```{r, message = FALSE, warning = FALSE}
rm(list = ls())
#Add libraries as needed
library(tidyverse)
library(randomForest)
library(lubridate)
library(glmnet)
library(FNN)
library(e1071)
```

## Call of Duty

![CODGames](Call-of-Duty.jpg-d196774.png)

```{r}
# Load datasets
P1 <- read_csv(file = "CODGames_p1_380.csv", show_col_types = FALSE)
head(P1)

P2 <- read_csv("CODGames_p2_380.csv", show_col_types = FALSE)
head(P2)

Maps <- read_csv("CODMaps.csv")
head(Maps)

Modes <- read_csv("CODGameModes.csv")
Modes
```

```{r}
# Appending Player 2 df to Player 1 df
Players <- P1 %>% 
  rbind(P2)
head(Players)
```

## Task 1 (Data Cleaning and Visualization)

Research Question: Which maps are the most likely to win the map vote? 

Since this question is centered around the probability of a map winning the map vote, we will not consider those maps who have null values in either the Map1 or Map2 columns, this includes the games which the player entered partway through. We want to judge only the rows where a vote for which map to play took place, therefore we will first omit any NA values in the Map1 or Map2 columns at the start of our analysis as well as any NA values in the Choice column. We will then get to know our data by listing out each unique map name in Choice, Map1, and Map2 and comparing them to the actual names found in our Maps data frame. If we detect any discrepancies we will have to account for this in our data cleaning step. Once each name is cleaned, we can perform the calculations across the variables. We will count the number of times a map name appears in both Map1 and Map2, the amount of times they were chosen in both Map1 and Map2, the number of Draws of Map Vote and which columns this took place in. It is incredibly important throughout this process that order be maintained throughout the rows so that when we eventually merge the Draw vector, it will be placed in the proper rows where a vote draw occured. From this information we will be able to subtract 'wins' from those maps in the Map1 row where a draw occured and discount losses for the maps in Map2 column where a draw occured. Finally, we'll be able to calculate the proportion of times a map won the map vote by getting more votes than the other and glean from that information the maps most likely to win the map vote.

```{r}
sum(is.na(Players$Choice))
```
we see that there are only 2 NA values, so most of our data is present.

```{r}
unique(Players$Choice)
unique(Maps$Name)
```

We can see that there are more unique map names in our Players dataframe as compared with the list of all the map names from our Maps dataframe. This shows us there are misspellings in the map names in the Players dataframe that we must correct before calculating summary statistics.

```{r}
Maps %>% 
  full_join(Players, by = c("Name" = "Choice")) %>% 
  group_by(Name) %>% 
  summarize(Count = n())
```


Using this join query, we can see there are multiple misspellings, added spaces, and a couple NA values. We shall correct for all this before running summaries.

Before running summary statistics, we must clean the data contained within the Choice and Map variables

```{r}
# Choice Data Cleaning
# remove NAs from Choice since there are only 2
Players <- Players %>% 
  drop_na(Choice) %>%  
  drop_na(Map1) %>% 
  drop_na(Map2)
# Remove trailing or leading spaces
Players <- Players %>% 
  mutate(Choice = trimws(Choice),
         Map1 = trimws(Map1),
         Map2 = trimws(Map2))

# Define a list of misspellings and their correct spellings and specify certain spellings for words with spaces in their names with '\\W'
misspellings <- list("APocalypse" = "Apocalypse", "Apocolypse" = "Apocalypse", "\\bCollaterel Strike\\b" = "Collateral", "\\bCollateral Strike\\b" = "Collateral", "Deisel" = "Diesel", "Drive-in" = "Drive-In", "\\bMiami Strike\\b" = "MiamiStrike", "\\bNuketown\\b.*" = "Nuketown", "Riad" = "Raid")

# Loop through the misspellings list and apply regex to correct the misspellings
for (misspelling in names(misspellings)) {
  correction <- misspellings[[misspelling]]
  Players$Choice <- gsub(misspelling, correction, Players$Choice, ignore.case = TRUE)
}
# Run new query to see if misspellings were corrected
Players %>% 
  group_by(Choice) %>% 
  summarise(Count = n())
```

We ended up changing some map names, e.g., "Collateral Strike" is now just "Collateral" and "Miami Strike" is "MiamiStrike". We needed to make these map names single words because otherwise the loop would not process or recognize the names correctly once they were altered.

Our misspellings are corrected and we can perform this data cleaning process on Map1 and Map2 variables now

```{r}
sum(is.na(Players$Map1))
sum(is.na(Players$Map2))
Players %>% 
  filter(FullPartial == "Partial") %>% 
  summarize(Count = n())
```

Both Map1 and Map2 variables have 163 missing values even though there are only 80 Partial games in the dataset. Therefore, we shouldn't remove the rows with NA values because those rows still have Choice values. Let's see if we can find typos in the map names by comparing them with the Map table

```{r}
Maps %>% 
  full_join(Players, by = c("Name" = "Map1")) %>% 
  group_by(Name) %>% 
  summarize(Count = n())
```

Since there are new typos, we just need to adjust our misspellings list to account for these. Note, we use the '\\b' encasing to denote we want to only replace the value when it matches exactly that spelling.

```{r}
# Map1 Data Cleaning
# Define a list of misspellings and their correct spellings and specify certain spellings for words with spaces in their names with '\\b'
misspellings <- list("\\bCollateral Striek\\b" = "Collateral", "\\bCollateral Strike\\b" = "Collateral", "\\bCollaterol Strike\\b" = "Collateral", "Deprogam" = "Deprogram", "Drive-in" = "Drive-In", "\\bMiami Stirke\\b" = "MiamiStrike", "\\bMiami Strike\\b" = "MiamiStrike", "\\bNuketown '84\\b" = "Nuketown", "Riad" = "Raid", "Ruah" = "Rush")

# Loop through the misspellings list and apply regex to correct the misspellings
for (misspelling in names(misspellings)) {
  correction <- misspellings[[misspelling]]
  Players$Map1 <- gsub(misspelling, correction, Players$Map1, ignore.case = TRUE)
}
# Run new query to see if misspellings were corrected
Players %>% 
  group_by(Map1) %>% 
  summarise(Count = n())
```

Great, now we just have to clean the names of Map2 and we can begin performing visualizations and running summary statistics.

```{r}
Maps %>% 
  full_join(Players, by = c("Name" = "Map2")) %>% 
  group_by(Name) %>% 
  summarize(Count = n())
```

```{r}
# Map2 Data Cleaning
# Define a list of misspellings and their correct spellings
misspellings <- list("\\bAmrada Strike\\b" = "Armada Strike", "\\bCollateral Strike\\b" = "Collateral", "Drive-in" = "Drive-In", "\\bMiami Sstrike\\b" = "MiamiStrike", "\\bMiami Stirke\\b" = "MiamiStrike", "\\bMiami Strike\\b" = "MiamiStrike", "\\bNuketown\\b.*" = "Nuketown", "\\byamantau\\b" = "Yamantau")

# Loop through the misspellings list and apply regex to correct the misspellings
for (misspelling in names(misspellings)) {
  correction <- misspellings[[misspelling]]
  Players$Map2 <- gsub(misspelling, correction, Players$Map2, ignore.case = TRUE)
}
# Run new query to see if misspellings were corrected
Players %>% 
  group_by(Map2) %>% 
  summarise(Count = n())
```

Now that our data is cleaned and variables match each other, we can begin visualizing the data.

```{r}
# Visualization of Choice
Players %>% 
  ggplot(aes(x = Choice)) +
  geom_histogram(stat = "count") +
  labs(title = "Distribution of Choice",
       x = "Choice",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


```{r}
# Visualization of Map 1
Players %>% 
  ggplot(aes(x = Map1)) +
  geom_histogram(stat = "count") +
  labs(title = "Distribution of Map 1",
       x = "Map",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

# Visualization of Map 2
Players %>% 
  ggplot(aes(x = Map2)) +
  geom_histogram(stat = "count", na.rm = TRUE) +
  labs(title = "Distribution of Map 2",
       x = "Map",
       y = "Count") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

From these visualizations, we can verify that the map names do not change but the distributions do. Our data cleaning is complete and we can now move onto calculating proportions.

Now we need to work with the MapVote variable and identify when a tie takes place. This is important, because whenever there is a tie Map1 gets chosen by default. We will first alter the MapVote variable into a numerical form where we can compare the two numbers in the column between eachother and store each time the two numbers are equivalent.

```{r}
# Remove NAs from Mapvote
Map_vote <- Players$MapVote
#Map_vote <- na.omit(Map_vote)
# Split Mapvote
Vote_split <- strsplit(Map_vote, "to")

# Remove NA values
# Vote_split <- na.omit(Vote_split) <- Don't know if this is necessary

# Initialize storage
First_vote <- rep(NA, length(Vote_split))
Second_vote <- rep(NA, length(Vote_split))
Draw <- rep(NA, length(Vote_split))

# Loop through Vote_split and store first and second vote numbers
for (i in 1:length(Vote_split)) {
  First_vote[i] <- (Vote_split[[i]][1])
  Second_vote[i] <- (Vote_split[[i]][2])
}

print(First_vote[500:600])
print(which(First_vote == "4 o 0"))
print(which(First_vote == "2 o 0"))
print(length(First_vote))
print(length(Second_vote))
```

We see that there are some entries that do not represent numbers that will be unsuccessfully converted to numeric values if we were to convert them now. We must impute these values and figure out there true meaning. The two entries in question from our vector First_vote are "4 o 0" and "2 o 0". This relates to a fundamental problem in our strsplit initially. The word "to" is misspelled as "o", meaning that these numbers will not be recognized by our split. This doesn't cause us an issue in this case because we are only concerned with the votes that ended up being ties and it seems like the votes were "2 to 0" and "4 to 0" respectively. This is something to note at the very least, let's now check our Second_vote vector to see the values before we convert them to numeric.

Now we know we must fix our strsplit function to cover these two cases.


```{r}
# Split Mapvote by "to" or "o"
Vote_split <- strsplit(Map_vote, "to|o")

# Remove NA values
# Vote_split <- na.omit(Vote_split) <- Don't know if this is necessary

# Initialize storage
First_vote <- rep(NA, length(Vote_split))
Second_vote <- rep(NA, length(Vote_split))
Draw <- rep(NA, length(Vote_split))

# Loop through Vote_split and store first and second vote numbers
for (i in 1:length(Vote_split)) {
  First_vote[i] <- (Vote_split[[i]][1])
  Second_vote[i] <- (Vote_split[[i]][2])
}

print(First_vote[500:600])
print(which(First_vote == "4 o 0"))
print(which(First_vote == "2 o 0"))
print(length(First_vote))
print(length(Second_vote))
```

Now we can't find the previous misspelled values and our number of rows are still same for our vectors.

We can compare the two vectors and count the number of times they match which would be equivalent to a draw. It is important that we keep the original order of our data frame so that we can know which rows ended in draws. We can then append the draw vector to our original data frame.

We now arrive at the number of times there was a draw in map votes between two maps. We know that whenever this occurs, Map1 is chosen by default. We can take this into consideration during our calculations.

Whenever the map vote results in a tie, we should discard the result of Choice and not consider it as a win. We need to loop through all rows in the data frame Players and compare the map vote totals. If there is a tie, we must note this and discard Choice, otherwise, the Choice correclty depicts the winner. We must make proportions, by summing up the total number of times a map was present in either Map1 or Map2 and its winning percentage. We may or may not discard the NA values of Map1 and Map2 because we don't know which maps the winner was competing against, but let's see.

It is important that we maintain order in our vector Draw so that we can append it to the Players dataframe and have it correctly represent the row that ends in a draw.

```{r}
# Convert to numeric for comparison sake
First_vote <- as.numeric(First_vote)
Second_vote <- as.numeric(Second_vote)
# Loop through Vote split and store 1 to Draw for every equal vote total, 0 otherwise
for (i in 1:length(Vote_split)) {
  # If the first vote has NA values, the second vote vector will as well
  if (is.na(First_vote[i])) {
    Draw[i] <- 0
  }
  # If the numbers in first vote and second vote are equivalent, then store a 1 in Draw vector
  else if (First_vote[i] == Second_vote[i]) {
    Draw[i] <- 1
  }
  else {
    Draw[i] <- 0
  }
}
sum(Draw)
```

We get 103 for the sum in Draw which is the same as our previous calculations, telling us that our loop worked and we can now add this Draw vector to the Players data frame and begin comparison.

```{r}
Players <-
  Players %>% 
  mutate(Draw = Draw)

head(Players)
```

After adding the Draw vector, we see it correctly indicates a 1 in the 4th row where there is a draw in the MapVote column.

```{r}
MapVoteWin <- Players %>%
  group_by(Map1) %>%
  summarize(Map1_Count = n(), Choice_M1 = sum(Choice == Map1), Draw_M1 = sum(Draw), Real_Win = Choice_M1 - Draw_M1) %>%
  left_join(
    Players %>%
      group_by(Map2) %>%
      summarize(Map2_Count = n(), Choice_M2 = sum(Choice == Map2), Draw_M2 = sum(Draw), Real_Loss = Map2_Count - Draw_M2),
    by = c("Map1" = "Map2")
  ) %>%
  mutate(
    Total_Count = Map1_Count + Map2_Count,
    Total_Choice = Choice_M1 + Choice_M2,
    Total_Draw = Draw_M1 + Draw_M2,
    Prop_Choice = Total_Choice / Total_Count,
    Adjusted_Choice_Count = Real_Win + Choice_M2,
    Adjusted_Prop_Choice = Adjusted_Choice_Count / (Map1_Count + Real_Loss)
  ) %>% 
  rename(Map = Map1) %>% 
  na.omit()
MapVoteWin
```

```{r}
MapVoteWin %>% 
  select(Map, Total_Count, Total_Choice, Total_Draw, Real_Win, Real_Loss, Adjusted_Choice_Count, Prop_Choice, Adjusted_Prop_Choice) %>% 
  arrange(desc(Adjusted_Prop_Choice))
```

From this analysis, we can see that the 3 maps most likely to win the map vote are Raid, Nuketown, and Crossroads Strike with true winning percentages of 75.6%, 73.7%, and 73.2% respectively. Nuketown actually had the highest proportion of Choice selection, with 82.5%, however it drew in map voting 5 times when it was Map1 and thus was selected 5 times by default when there was a tie in voting, greatly boosting its Choice proportion. This is why we created the variables of 'Real_Win' and 'Real_Loss' to account for the number of times a map in Map1 column truly won the map vote and not by default and the number of times a map in Map2 actually lost the map vote and not lost by default because of a tie.

We can visualze this data
```{r}
MapVoteWin %>% 
  ggplot(aes(x = Map, fill = "dodgerblue")) +
  geom_bar(aes(weight = Adjusted_Prop_Choice), show.legend = F) +
  scale_fill_manual(values = c("dodgerblue")) +
  labs(title = "Probability to Win Map Vote",
       x = "Map",
       y = "Proportion of Map Vote Win") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

(maybe add more visualizations)

# Task 2

```{r}

# Clean the data to clear to distinction between HC-TDM/Hardpoint and TDM/Hardpoint

Players <- 
  Players %>%
  mutate(GameType = str_replace(GameType, "HC - ", "")) 

# Check to see if data was properly cleaned and if there are any NAs
Players %>% 
  group_by(GameType) %>% 
  summarise(Count = n())

```


The GameType column has been cleaned so we can now start an exploratory data analysis. 


```{r}
ggplot(Players,
       aes(x = Score, y = TotalXP, color = GameType)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = "lm", se = F) +
  labs(title = "Total experience points vs Score by Game Type",
       y = "Total experience points")

```



```{r}
ggplot(Players, aes(x = GameType, y = TotalXP)) +
  geom_boxplot(fill = "blue", color = "black") +
  labs(title = "Distribution of Total experience points by Game Type",
       x = "Game Type")

ggplot(Players, aes(x = GameType, y = Score)) +
  geom_boxplot(fill = "blue", color = "black") +
  labs(title = "Distribution of Score by Game Type",
       x = "Game Type")

```

```{r}

```



```{r}
model1 <- lm(TotalXP ~ Score + GameType, data = Players)
summary(model1)

```


## Task 3

Which Primary Weapon is best in terms of Eliminations? (If we were given a player's stat line, can we know which weapon they used?)
Can we predict weapon type based on several factors?

To answer this research question, we will have to account for factors such as which game type because each types has different lengths . We will go by average eliminations for each weapon and account for factors such as game type which can have different lengths. We will have to perform some data cleaning initially to make sure each weapon is accounted for and does not have typos. We then will decide on a threshold of the number of games a weapon must have been used in to be considered, too few games will not be convincing enough. We will be removing partial matches from consideration since we are analyzing a counting statistic 'Eliminations'. When comparing the different models, we must compare them on the same metric which will be accuracy. We will be using the same random seed of 123 throughout this model process to keep our results replicable and consistent throughout the models.

```{r}
# Remove partial matches
Players <-
  Players %>% 
  filter(FullPartial == "Full")
```


```{r}
# Summarize count of games each weapon appeared in
Players %>% 
  group_by(PrimaryWeapon) %>% 
  summarize(N = n())
```

We notice that there are some typos in a couple guns and multiple NA values which we must clean up before moving forward

```{r}
# Drop NA values
Players <- Players %>% 
  drop_na(PrimaryWeapon)

# Correct typos
misspellings <- list("\\bMilano 821\\b" = "Milano", "\\bPellington\\b" = "Pelington 703")

# Loop through the misspellings list and apply regex to correct the misspellings
for (misspelling in names(misspellings)) {
  correction <- misspellings[[misspelling]]
  Players$PrimaryWeapon <- gsub(misspelling, correction, Players$PrimaryWeapon, ignore.case = TRUE)
}

# View corrections
Players %>% 
  group_by(PrimaryWeapon) %>% 
  summarize(N = n())
```

Now we will select only the top 5 weapons which appear the most in our dataset to reduce the number of classes for our classifcation models.


```{r}
# Remove weapons with few game counts
Weapons <- Players %>% 
  group_by(PrimaryWeapon) %>% 
  filter(n() >= 40)

Weapons %>% 
  group_by(PrimaryWeapon) %>% 
  summarise(N = n())
```

```{r}
Weapons %>% 
  ggplot(aes(x = PrimaryWeapon, y = Eliminations)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

```{r}
Weapons %>% 
  ggplot(aes(x = Damage, y = Eliminations, color = PrimaryWeapon)) +
  geom_point() +
  facet_wrap( ~ GameType)
```

Let's make use of the date variable. We'll put it into a format that's useful, splitting it up by the month

```{r}
# Create Month variable
Weapons <-
  Weapons %>% 
    mutate(Month = month(as.POSIXlt(Date, format = "%m/%d/%Y"))) %>% 
  mutate(Month = month.name[Month])

# Convert to continuous variable
Weapons$Month <- as.integer(factor(Weapons$Month, levels = month.name))
```

Now we can plot the changes over time for our weapons. 

```{r}
Weapons %>%
  group_by(Month, PrimaryWeapon) %>%
  summarise(mean_eliminations = mean(Eliminations)) %>%
  ggplot(aes(x = Month, y = mean_eliminations, color = PrimaryWeapon)) +
  geom_line(aes(group = PrimaryWeapon)) +
  geom_point() +
  labs(x = "Month", y = "Eliminations", color = "Primary Weapon") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

### Feature Selection

Before we begin building our models, it would be beneficial to employ a feature selection model to help us determine which variables are most important in classifying our PrimaryWeapons. Let's build a LASSO model to help us identify those variables most important to predicting PrimaryWeapon. 

```{r}
# Create factor for PrimaryWeapon
Weapons$PrimaryWeapon <- factor(Weapons$PrimaryWeapon)

# Build LASSO model with most variables
formula <- as.formula("PrimaryWeapon ~ Eliminations + Deaths + TotalXP + Score + Damage + Month + GameType + Map1 + Map2 + Choice + XPType + GameType")

# Put data frame in form needed for glmnet
Xmat <- model.matrix(formula, data = Weapons)
y <- as.numeric(Weapons$PrimaryWeapon)

set.seed(123)
cv.out <- cv.glmnet(x = Xmat, y = y,
                    alpha = 1, standardize = TRUE,
                    nfolds = 10)
set.seed(NULL)
plot(cv.out)
```

Let's select the largest lambda value within 1 standard deviation of the minimum. This will provide us with a lambda value that will apply more regularization and thus narrow down our features to only the most important.

```{r}
bestlam1 <- cv.out$lambda.1se
#Predict the responses for the test set (use for MSE/RMSE calc)
lasso.pred1 <- predict(cv.out , s = bestlam1,
                      newx = Xmat)
#Find the coefficients
lasso.coef1 <- predict(cv.out , s = bestlam1,
                      type = "coefficients")
bestlam1
lasso.coef1
```

We see that the month variable dominates the model, while interestingly enough 'Eliminations' and 'Deaths' are discarded by this LASSO model telling us that they may not be that important. Let's try the model again but without the 'Month' or Map variables.

```{r}
# Build LASSO model
formula <- as.formula("PrimaryWeapon ~ Eliminations + Deaths + TotalXP + Score + Damage + GameType")

# Put data frame in form needed for glmnet
Xmat <- model.matrix(formula, data = Weapons)
y <- as.numeric(Weapons$PrimaryWeapon)

set.seed(123)
cv.out <- cv.glmnet(x = Xmat, y = y,
                    alpha = 1, standardize = TRUE,
                    nfolds = 10)
set.seed(NULL)

bestlam1 <- cv.out$lambda.1se
#Predict the responses for the test set (use for MSE/RMSE calc)
lasso.pred1 <- predict(cv.out , s = bestlam1,
                      newx = Xmat)
#Find the coefficients
lasso.coef1 <- predict(cv.out , s = bestlam1,
                      type = "coefficients")
bestlam1
lasso.coef1
```

We can see that the variables most important for predicting 'PrimaryWeapon' are Month, Eliminations, Score, Damage, and GameType based on this LASSO model.


### Random Forest Model

```{r}
# Create indicator variables for GameType and Month
Weapons <- Weapons %>%
  mutate(Month = factor(Month, levels = unique(Month)),
         GameType = factor(GameType, levels = unique(GameType))) %>% 
  mutate(Month_dummy = as.numeric(Month),
         Type_dummy = as.numeric(GameType)) # convert Month to numeric values

# Scale x variables
xvars <- c("Month_dummy", "Type_dummy", "Eliminations", "Score", "Damage")
Weapons[ , xvars] <- scale(Weapons[ , xvars], center = TRUE, scale = TRUE)
```


```{r}
# Generate Train and Test sets
set.seed(42)
train_ind <- sample(1:nrow(Weapons), floor(0.8 * nrow(Weapons)))
set.seed(NULL)

Train <- Weapons[train_ind, ]
Test <- Weapons[-train_ind, ]
```


```{r}
#Build model using a seed
set.seed(42)
rf <- randomForest(as.factor(PrimaryWeapon) ~ Eliminations + Score + Damage + GameType + Month, 
                   data = Train, 
                   ntree = 500, 
                   importance = TRUE)
set.seed(NULL)

#Obtain predicted probabilities - not need for confusion matrix but included to show we all get same probabilities with same seed
pred_prob <- predict(rf, newdata = Test, type = "prob")

#To create the confusion matrix, we need to determine which class (No or Yes) is more likely. Although we could define a threshold and compare probabilities as we have done in the past, things are more challenging if y has more than 2 classes. So, let R pick for us.
pred_surv <- predict(rf, newdata = Test, type = "response")

#Create confusion matrix
table(pred_surv, Test$PrimaryWeapon)

#Calculate accuracy
mean(pred_surv == Test$PrimaryWeapon)

rf$importance

plot(rf)

varImpPlot(rf, n.var = 5)
```

We achieve 87.7% accuracy based from our random forest model and see that 'Month' and 'Damage' are the most important variables for predicting 'PrimaryWeapon' based on the mean decrease in accuracy and Gini. This makes inuitive sense because each gun can have different damage settings and the player can have phases in their experience where they use one gun for a certain period of time before moving on. We also see that the error for our random forest model converges before 100 trees, showing that 500 trees aren't necessary. We can try and adjust the parameters of our random forest model to see if we can achieve a higher accuracy, but let's move onto another classification method first.

### KNN Classification

Our next classification method that we will implement is the k-Nearest Neighbors algorithm, which we covered in class. Since our data is already scaled and split into training and testing sets, we can jump right into the algorithm. We will also make sure to test the effectiveness of this model on the same statistic as our random forest model which was accuracy. However, we don't know which value of k gives us the highest accuracy. This is where we will implement a loop to find the optimal k value.

```{r}
# Initialize storage vars
maxK <- 75
acc_vec <- rep(NA, maxK)
actual <- Test$PrimaryWeapon  # actual values
# Loop
for (k in 1:maxK) {
  
#Build kNN classification model
knn_res <- knn(train = Train[ , xvars, drop = FALSE],
               test = Test[ , xvars, drop = FALSE],
               cl = Train$PrimaryWeapon,
               k = k)

# calculate accuracy of kNN model
predicted <- knn_res  # predicted values
# Convert factor levels to match those of Test$PrimaryWeapon
predicted <- factor(predicted, levels = levels(Test$PrimaryWeapon))

# Calculate and store accuracy
acc_vec[k] <- sum(predicted == Test$PrimaryWeapon) / length(Test$PrimaryWeapon)  # calculate accuracy
}
```

With our accuracies stored in acc_vec, we can plot the accuracies and find the optimal k value.

```{r}
# To use ggplot, create a data frame
temp_df <- data.frame(k = 1:maxK, accuracy = acc_vec)

# Create plot
ggplot(temp_df, aes(x = k, y = acc_vec)) +
  geom_line() +
  labs(x = "Number of Nearest Neighbors (k)",
       y = "Accuracy")

# Best K value
which.max(acc_vec)
# Best accuracy
acc_vec[which.max(acc_vec)]
```

The best accuracy achieved was 81.5% with a K value of 3. Our random forest model outperforms this kNN model by about 6% in terms of accuracy.

### Naive Bayes Classifier

We will implement a naive bayes classifier for our final model. A Naive Bayes works by calculating the probability of a new data point belonging to each class based on the probability of its features give each class. Just like we did with our kNN classifier, we will loop through an important hyperparameter looking to optimize the accuracy value of the model. For our naive bayes classifier, this hyperparameter is the laplace value which adjusts the amount of smoothing applied to the model - a higher value results in more smoothing.

```{r}
library(e1071)

# fit a naive bayes classifier to the training data
nb_model <- naiveBayes(as.factor(PrimaryWeapon) ~ Eliminations + Score + Damage + GameType + Month, data = Train)

# make predictions on the test data using the fitted model
nb_pred <- predict(nb_model, newdata = Test)

# calculate the accuracy of the model
nb_accuracy <- sum(nb_pred == Test$PrimaryWeapon) / nrow(Test)
```

```{r}
# Set up vector of laplace values to test
laplace_values <- seq(-2, 1, by = 0.1)

# Create an empty vector to store accuracy values
accuracy_values <- numeric(length = length(laplace_values))

# Loop through laplace values and fit naive bayes models
for (i in seq_along(laplace_values)) {
  nb_model <- naiveBayes(PrimaryWeapon ~ Eliminations + Score + Damage + GameType + Month, 
                         data = Train, 
                         laplace = laplace_values[i])
  
  # Make predictions on test set
  predicted <- predict(nb_model, newdata = Test)
  
  # Calculate accuracy
  accuracy_values[i] <- sum(predicted == Test$PrimaryWeapon) / length(predicted)
}

# Find the optimal laplace value based on highest accuracy
optimal_laplace <- laplace_values[which.max(accuracy_values)]

# Fit final model with optimal laplace value
nb_model <- naiveBayes(as.factor(PrimaryWeapon) ~ Eliminations + Score + Damage + GameType + Month, 
                       data = Train, 
                       laplace = optimal_laplace)

```

```{r}
# create a data frame with laplace_values and corresponding accuracies
acc_df <- data.frame(laplace_values, accuracy_values)

# plot accuracy vs. laplace_values
ggplot(acc_df, aes(x = laplace_values, y = accuracy_values)) + 
  geom_line() + 
  geom_point() + 
  labs(title = "Accuracy vs. Laplace Values", 
       x = "Laplace Values", 
       y = "Accuracy")

# Best Laplace value
which.max(accuracy_values)
# Best accuracy
accuracy_values[which.max(accuracy_values)]
```

We find that a Laplace value of -1 gives us the highest accuracy of 81.5% for our naive bayes model. There are multiple laplace values below -1 which give us this accuracy but we chose -1 to prevent possible overfitting if we were to implement this model on new data.
